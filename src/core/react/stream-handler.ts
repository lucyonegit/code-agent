/**
 * ReAct Streaming å¤„ç†é€»è¾‘
 */

import { AIMessage, type BaseMessage } from '@langchain/core/messages';
import { ChatOpenAI } from '@langchain/openai';
import {
  mergeToolCalls,
  toLangChainToolCalls,
  type AccumulatedToolCall,
  type ToolCallChunk,
} from '../utils/streamHelper.js';
import { type ReActInput } from '../../types/index.js';
import { type ReActLogger } from '../ReActLogger.js';

export interface StreamResult {
  content: string;
  toolCalls: Array<{ id?: string; name: string; args: Record<string, any> }>;
  message: AIMessage;
}

export class StreamHandler {
  constructor(
    private logger: ReActLogger,
    private onMessage?: ReActInput['onMessage']
  ) { }

  /**
   * è¯»å–æµå¹¶è¿”å›ç´¯ç§¯ç»“æœ
   * ä¸å†è´Ÿè´£å·¥å…·æ‰§è¡Œ
   */
  async readStream(
    llm: ReturnType<ChatOpenAI['bindTools']>,
    messages: BaseMessage[],
    iterationId: string
  ): Promise<StreamResult> {
    const stream = await llm.stream(messages);

    // ç´¯ç§¯å†…å®¹å’Œå·¥å…·è°ƒç”¨
    let accumulatedContent = '';
    let accumulatedToolCalls: AccumulatedToolCall[] = [];

    /** @type {AccumulatedToolCall[]} */

    // å¤„ç†æµå¼æ•°æ®Thought & ToolCall
    for await (const chunk of stream) {
      // é˜¶æ®µ 1: Thought æµå¼è¾“å‡º
      if (chunk.content) {
        const text = typeof chunk.content === 'string' ? chunk.content : '';
        if (text) {
          // TRACE çº§åˆ«ï¼šæµå¼ chunk è¾“å‡º
          this.logger.streamChunk(text);
          accumulatedContent += text;
          await this.emitEvent({
            type: 'thought',
            thoughtId: iterationId,
            chunk: text,
            isComplete: false,
            timestamp: Date.now(),
          });

          // äººå·¥å»¶æ—¶ï¼šæ¨¡æ‹Ÿæµå¼æ•ˆæœï¼ˆå›  LiteLLM ä»£ç†æ‰¹é‡è¿”å› chunksï¼‰
          await this.delay(30);
        }
      }



      // é˜¶æ®µ 2: Action ç´¯ç§¯
      if (chunk.tool_call_chunks && chunk.tool_call_chunks.length > 0) {
        accumulatedToolCalls = mergeToolCalls(
          accumulatedToolCalls,
          chunk.tool_call_chunks as ToolCallChunk[]
        );
      }
    }

    if (accumulatedContent) {
      this.logger.debug('ğŸ§  æµå¼æ€è€ƒå®Œæˆ', {
        contentLength: accumulatedContent.length,
        contentPreview: accumulatedContent.slice(0, 100),
      });
      await this.emitEvent({
        type: 'thought',
        thoughtId: iterationId,
        chunk: '',
        isComplete: true,
        timestamp: Date.now(),
      });
    }

    // æ„å»º AI æ¶ˆæ¯å¹¶æ·»åŠ åˆ°å†å²
    const toolCalls = toLangChainToolCalls(accumulatedToolCalls);

    // DEBUGï¼šæ‰“å°è§£æåçš„å·¥å…·è°ƒç”¨
    if (toolCalls.length > 0) {
      this.logger.debug('ğŸ”§ è§£æå·¥å…·è°ƒç”¨', {
        count: toolCalls.length,
        tools: toolCalls.map(tc => ({
          name: tc.name,
          argsLength: JSON.stringify(tc.args).length,
        })),
      });
    }

    const aiMessage = new AIMessage({
      content: accumulatedContent,
      tool_calls: toolCalls.length > 0 ? toolCalls : undefined,
    });

    // å‘é€ assistant æ¶ˆæ¯åŒæ­¥äº‹ä»¶
    const assistantMsgId = `msg_${Date.now()}_${Math.random().toString(36).slice(2, 9)}`;
    await this.emitEvent({
      type: 'message_sync',
      message: {
        id: assistantMsgId,
        role: 'assistant',
        content: accumulatedContent,
        toolCalls: toolCalls.map(tc => ({
          id: tc.id || `call_${Date.now()}`,
          name: tc.name,
          args: tc.args as Record<string, unknown>,
        })),
        timestamp: Date.now(),
      },
      timestamp: Date.now(),
    });

    return {
      content: accumulatedContent,
      toolCalls: toolCalls.map(tc => ({
        id: tc.id,
        name: tc.name,
        args: tc.args as Record<string, any>,
      })),
      message: aiMessage,
    };
  }

  private async emitEvent(event: any): Promise<void> {
    if (this.onMessage) {
      await this.onMessage(event);
    }
  }

  private delay(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }
}

